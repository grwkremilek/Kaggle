{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\n",
      "/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\n",
      "/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\n",
      "/kaggle/input/tweetsdatacleaning/__output__.json\n",
      "/kaggle/input/tweetsdatacleaning/cleaned_all.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import operator \n",
    "\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/tweetsdatacleaning/cleaned_all.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "df = pd.concat([train,test], sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/philculliton/nlp-getting-started-tutorial\n",
    "# CREATE VECTORS\n",
    "# Using just .transform() with the test data makes sure that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "#vectors = count_vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    ''' go through all text and count the occurance of the contained words '''\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    ''' check the intersection between vocabulary and embeddings '''\n",
    "    a = {}; oov = {}\n",
    "    k = 0; i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "Kosciusko police investigating pedestrian fatality hit by a train Thursday                                                                 2\n",
       "Petition  Heartless owner that whipped horse until it collapsed is told he can KEEP his animal Act Now                                     2\n",
       "I liked a YouTube video from itsjustinstuart  GUN RANGE MAYHEM                                                                             2\n",
       "Who is bringing the tornadoes and floods Who is bringing the climate change God is after America He is plaguing her\\n \\nFARRAKHAN QUOTE    2\n",
       "hot  C130 specially modified to land in a stadium and rescue hostages in Iran in 1980  prebreak best                                       2\n",
       "                                                                                                                                          ..\n",
       "World Annihilation vs Self Transformation  Aliens Attack to Exterminate Humans                                                             2\n",
       "Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs An angry Internally Displaced wom                              2\n",
       "World War II book LIGHTNING JOE An Autobiography by General J Lawton Collins                                                               2\n",
       "Caution breathing may be hazardous to your health                                                                                          2\n",
       "Pandemonium In Aba As Woman Delivers Baby Without Face Photos                                                                              2\n",
       "Name: target, Length: 72, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check and correct mislabelled tweets\n",
    "df_mislabeled = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
    "df_mislabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_relabeled'] = df['target'].copy() \n",
    "\n",
    "df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "df.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "df.loc[df['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Our': 43, 'Deeds': 2, 'are': 678, 'the': 4696, 'Reason': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"text\"].apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):    \n",
    "    # Punctuations at the start or end of words    \n",
    "    for punctuation in \"#@!?()[]*%\":\n",
    "        tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n",
    "    tweet = tweet.replace('...', ' ... ').strip()\n",
    "    tweet = tweet.replace(\"'\", \" ' \").strip()        \n",
    "    \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "        \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"chest/torso\", \"chest / torso\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    \n",
    "    # Separating other punctuations\n",
    "    tweet = re.sub(r\"MH370:\", \"MH370 :\", tweet)\n",
    "    tweet = re.sub(r\"PM:\", \"Prime Minister :\", tweet)\n",
    "    tweet = re.sub(r\"Legionnaires:\", \"Legionnaires :\", tweet)\n",
    "    tweet = re.sub(r\"Latest:\", \"Latest :\", tweet)\n",
    "    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n",
    "    tweet = re.sub(r\"News:\", \"News :\", tweet)\n",
    "    tweet = re.sub(r\"derailment:\", \"derailment :\", tweet)\n",
    "    tweet = re.sub(r\"attack:\", \"attack :\", tweet)\n",
    "    tweet = re.sub(r\"Saipan:\", \"Saipan :\", tweet)\n",
    "    tweet = re.sub(r\"Photo:\", \"Photo :\", tweet)\n",
    "    tweet = re.sub(r\"Funtenna:\", \"Funtenna :\", tweet)\n",
    "    tweet = re.sub(r\"quiz:\", \"quiz :\", tweet)\n",
    "    tweet = re.sub(r\"VIDEO:\", \"VIDEO :\", tweet)\n",
    "    tweet = re.sub(r\"MP:\", \"MP :\", tweet)\n",
    "    tweet = re.sub(r\"UTC2015-08-05\", \"UTC 2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"California:\", \"California :\", tweet)\n",
    "    tweet = re.sub(r\"horror:\", \"horror :\", tweet)\n",
    "    tweet = re.sub(r\"Past:\", \"Past :\", tweet)\n",
    "    tweet = re.sub(r\"Time2015-08-06\", \"Time 2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"here:\", \"here :\", tweet)\n",
    "    tweet = re.sub(r\"fires.\", \"fires .\", tweet)\n",
    "    tweet = re.sub(r\"Forest:\", \"Forest :\", tweet)\n",
    "    tweet = re.sub(r\"Cramer:\", \"Cramer :\", tweet)\n",
    "    tweet = re.sub(r\"Chile:\", \"Chile :\", tweet)\n",
    "    tweet = re.sub(r\"link:\", \"link :\", tweet)\n",
    "    tweet = re.sub(r\"crash:\", \"crash :\", tweet)\n",
    "    tweet = re.sub(r\"Video:\", \"Video :\", tweet)\n",
    "    tweet = re.sub(r\"Bestnaijamade:\", \"bestnaijamade :\", tweet)\n",
    "    tweet = re.sub(r\"NWS:\", \"National Weather Service :\", tweet)\n",
    "    tweet = re.sub(r\".caught\", \". caught\", tweet)\n",
    "    tweet = re.sub(r\"Hobbit:\", \"Hobbit :\", tweet)\n",
    "    tweet = re.sub(r\"2015:\", \"2015 :\", tweet)\n",
    "    tweet = re.sub(r\"post:\", \"post :\", tweet)\n",
    "    tweet = re.sub(r\"BREAKING:\", \"BREAKING :\", tweet)\n",
    "    tweet = re.sub(r\"Island:\", \"Island :\", tweet)\n",
    "    tweet = re.sub(r\"Med:\", \"Med :\", tweet)\n",
    "    tweet = re.sub(r\"97/Georgia\", \"97 / Georgia\", tweet)\n",
    "    tweet = re.sub(r\"Here:\", \"Here :\", tweet)\n",
    "    tweet = re.sub(r\"horror;\", \"horror ;\", tweet)\n",
    "    tweet = re.sub(r\"people;\", \"people ;\", tweet)\n",
    "    tweet = re.sub(r\"refugees;\", \"refugees ;\", tweet)\n",
    "    tweet = re.sub(r\"Genocide;\", \"Genocide ;\", tweet)\n",
    "    tweet = re.sub(r\".POTUS\", \". POTUS\", tweet)\n",
    "    tweet = re.sub(r\"Collision-No\", \"Collision - No\", tweet)\n",
    "    tweet = re.sub(r\"Rear-\", \"Rear -\", tweet)\n",
    "    tweet = re.sub(r\"Broadway:\", \"Broadway :\", tweet)\n",
    "    tweet = re.sub(r\"Correction:\", \"Correction :\", tweet)\n",
    "    tweet = re.sub(r\"UPDATE:\", \"UPDATE :\", tweet)\n",
    "    tweet = re.sub(r\"Times:\", \"Times :\", tweet)\n",
    "    tweet = re.sub(r\"RT:\", \"RT :\", tweet)\n",
    "    tweet = re.sub(r\"Police:\", \"Police :\", tweet)\n",
    "    tweet = re.sub(r\"Training:\", \"Training :\", tweet)\n",
    "    tweet = re.sub(r\"Hawaii:\", \"Hawaii :\", tweet)\n",
    "    tweet = re.sub(r\"Selfies:\", \"Selfies :\", tweet)\n",
    "    tweet = re.sub(r\"Content:\", \"Content :\", tweet)\n",
    "    tweet = re.sub(r\"101:\", \"101 :\", tweet)\n",
    "    tweet = re.sub(r\"story:\", \"story :\", tweet)\n",
    "    tweet = re.sub(r\"injured:\", \"injured :\", tweet)\n",
    "    tweet = re.sub(r\"poll:\", \"poll :\", tweet)\n",
    "    tweet = re.sub(r\"Guide:\", \"Guide :\", tweet)\n",
    "    tweet = re.sub(r\"Update:\", \"Update :\", tweet)\n",
    "    tweet = re.sub(r\"alarm:\", \"alarm :\", tweet)\n",
    "    tweet = re.sub(r\"floods:\", \"floods :\", tweet)\n",
    "    tweet = re.sub(r\"Flood:\", \"Flood :\", tweet)\n",
    "    tweet = re.sub(r\"MH370;\", \"MH370 ;\", tweet)\n",
    "    tweet = re.sub(r\"life:\", \"life :\", tweet)\n",
    "    tweet = re.sub(r\"crush:\", \"crush :\", tweet)\n",
    "    tweet = re.sub(r\"now:\", \"now :\", tweet)\n",
    "    tweet = re.sub(r\"Vote:\", \"Vote :\", tweet)\n",
    "    tweet = re.sub(r\"Catastrophe.\", \"Catastrophe .\", tweet)\n",
    "    tweet = re.sub(r\"library:\", \"library :\", tweet)\n",
    "    tweet = re.sub(r\"Bush:\", \"Bush :\", tweet)\n",
    "    tweet = re.sub(r\";ACCIDENT\", \"; ACCIDENT\", tweet)\n",
    "    tweet = re.sub(r\"accident:\", \"accident :\", tweet)\n",
    "    tweet = re.sub(r\"Taiwan;\", \"Taiwan ;\", tweet)\n",
    "    tweet = re.sub(r\"Map:\", \"Map :\", tweet)\n",
    "    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n",
    "    tweet = re.sub(r\"150-Foot\", \"150 - Foot\", tweet)\n",
    "    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n",
    "    tweet = re.sub(r\"prefer:\", \"prefer :\", tweet)\n",
    "    tweet = re.sub(r\"CNN:\", \"CNN :\", tweet)\n",
    "    tweet = re.sub(r\"Oops:\", \"Oops :\", tweet)\n",
    "    tweet = re.sub(r\"Disco:\", \"Disco :\", tweet)\n",
    "    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n",
    "    tweet = re.sub(r\"Grows:\", \"Grows :\", tweet)\n",
    "    tweet = re.sub(r\"projected:\", \"projected :\", tweet)\n",
    "    tweet = re.sub(r\"Pakistan.\", \"Pakistan .\", tweet)\n",
    "    tweet = re.sub(r\"ministers:\", \"ministers :\", tweet)\n",
    "    tweet = re.sub(r\"Photos:\", \"Photos :\", tweet)\n",
    "    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n",
    "    tweet = re.sub(r\"pres:\", \"press :\", tweet)\n",
    "    tweet = re.sub(r\"winds.\", \"winds .\", tweet)\n",
    "    tweet = re.sub(r\"MPH.\", \"MPH .\", tweet)\n",
    "    tweet = re.sub(r\"PHOTOS:\", \"PHOTOS :\", tweet)\n",
    "    tweet = re.sub(r\"Time2015-08-05\", \"Time 2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"Denmark:\", \"Denmark :\", tweet)\n",
    "    tweet = re.sub(r\"Articles:\", \"Articles :\", tweet)\n",
    "    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n",
    "    tweet = re.sub(r\"casualties.:\", \"casualties .:\", tweet)\n",
    "    tweet = re.sub(r\"Afghanistan:\", \"Afghanistan :\", tweet)\n",
    "    tweet = re.sub(r\"Day:\", \"Day :\", tweet)\n",
    "    tweet = re.sub(r\"AVERTED:\", \"AVERTED :\", tweet)\n",
    "    tweet = re.sub(r\"sitting:\", \"sitting :\", tweet)\n",
    "    tweet = re.sub(r\"Multiplayer:\", \"Multiplayer :\", tweet)\n",
    "    tweet = re.sub(r\"Kaduna:\", \"Kaduna :\", tweet)\n",
    "    tweet = re.sub(r\"favorite:\", \"favorite :\", tweet)\n",
    "    tweet = re.sub(r\"home:\", \"home :\", tweet)\n",
    "    tweet = re.sub(r\"just:\", \"just :\", tweet)\n",
    "    tweet = re.sub(r\"Collision-1141\", \"Collision - 1141\", tweet)\n",
    "    tweet = re.sub(r\"County:\", \"County :\", tweet)\n",
    "    tweet = re.sub(r\"Duty:\", \"Duty :\", tweet)\n",
    "    tweet = re.sub(r\"page:\", \"page :\", tweet)\n",
    "    tweet = re.sub(r\"Attack:\", \"Attack :\", tweet)\n",
    "    tweet = re.sub(r\"Minecraft:\", \"Minecraft :\", tweet)\n",
    "    tweet = re.sub(r\"wounds;\", \"wounds ;\", tweet)\n",
    "    tweet = re.sub(r\"Shots:\", \"Shots :\", tweet)\n",
    "    tweet = re.sub(r\"shots:\", \"shots :\", tweet)\n",
    "    tweet = re.sub(r\"Gunfire:\", \"Gunfire :\", tweet)\n",
    "    tweet = re.sub(r\"hike:\", \"hike :\", tweet)\n",
    "    tweet = re.sub(r\"Email:\", \"Email :\", tweet)\n",
    "    tweet = re.sub(r\"System:\", \"System :\", tweet)\n",
    "    tweet = re.sub(r\"Radio:\", \"Radio :\", tweet)\n",
    "    tweet = re.sub(r\"King:\", \"King :\", tweet)\n",
    "    tweet = re.sub(r\"upheaval:\", \"upheaval :\", tweet)\n",
    "    tweet = re.sub(r\"tragedy;\", \"tragedy ;\", tweet)\n",
    "    tweet = re.sub(r\"HERE:\", \"HERE :\", tweet)\n",
    "    tweet = re.sub(r\"terrorism:\", \"terrorism :\", tweet)\n",
    "    tweet = re.sub(r\"police:\", \"police :\", tweet)\n",
    "    tweet = re.sub(r\"Mosque:\", \"Mosque :\", tweet)\n",
    "    tweet = re.sub(r\"Rightways:\", \"Rightways :\", tweet)\n",
    "    tweet = re.sub(r\"Brooklyn:\", \"Brooklyn :\", tweet)\n",
    "    tweet = re.sub(r\"Arrived:\", \"Arrived :\", tweet)\n",
    "    tweet = re.sub(r\"Home:\", \"Home :\", tweet)\n",
    "    tweet = re.sub(r\"Earth:\", \"Earth :\", tweet)\n",
    "    tweet = re.sub(r\"three:\", \"three :\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    \n",
    "    # BrEn x AmEn\n",
    "    tweet = re.sub(r\"colour\", \"color\", tweet)\n",
    "    tweet = re.sub(r\"centre\", \"center\", tweet)\n",
    "    tweet = re.sub(r\"favourite\", \"favorite\", tweet)\n",
    "    tweet = re.sub(r\"travelling\", \"traveling\", tweet)\n",
    "    tweet = re.sub(r\"counselling\", \"counseling\", tweet)\n",
    "    tweet = re.sub(r\"theatre\", \"theater\", tweet)\n",
    "    tweet = re.sub(r\"cancelled\", \"canceled\", tweet)\n",
    "    tweet = re.sub(r\"labour\", \"labor\", tweet)\n",
    "    tweet = re.sub(r\"organisation\", \"organization\", tweet)\n",
    "    tweet = re.sub(r\"instagram\", \"social medium\", tweet)\n",
    "    tweet = re.sub(r\"whatsapp\", \"social medium\", tweet)\n",
    "    tweet = re.sub(r\"snapchat\", \"social medium\", tweet)\n",
    "    \n",
    "    if 'http' not in tweet:\n",
    "        tweet = tweet.replace(\":\", \" : \").strip() \n",
    "        tweet = tweet.replace(\".\", \" . \").strip()           \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "W2V_path = '../embeddings/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(W2V_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 54.32% of vocab\n",
      "Found embeddings for  81.80% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 3438),\n",
       " ('a', 3419),\n",
       " ('of', 3232),\n",
       " ('and', 2411),\n",
       " ('-', 300),\n",
       " ('MH370', 99),\n",
       " ('2015', 97),\n",
       " ('\\x89ÛÒ', 93),\n",
       " ('??', 90),\n",
       " ('\\x89Û', 90),\n",
       " ('70', 74),\n",
       " ('traumatised', 59),\n",
       " ('15', 59),\n",
       " ('40', 59),\n",
       " ('|', 58),\n",
       " ('\\x89ÛÓ', 58),\n",
       " ('16yr', 56),\n",
       " ('...', 55),\n",
       " ('30', 52),\n",
       " ('60', 45)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2Voov = check_coverage(vocab,embeddings_index)\n",
    "W2Voov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 62.87% of vocab\n",
      "Found embeddings for  81.87% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 3447),\n",
       " ('a', 3435),\n",
       " ('of', 3234),\n",
       " ('.', 2930),\n",
       " ('and', 2416),\n",
       " (\"'\", 1336),\n",
       " ('?', 1332),\n",
       " ('!', 494),\n",
       " (':', 486),\n",
       " ('...', 384),\n",
       " ('-', 307),\n",
       " (')', 173),\n",
       " ('(', 156),\n",
       " ('MH370', 114),\n",
       " ('2015', 112),\n",
       " ('\\x89Û', 90),\n",
       " ('70', 74),\n",
       " ('40', 64),\n",
       " ('traumatised', 63),\n",
       " ('[', 59)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: clean(x))\n",
    "\n",
    "sentences = df[\"text\"].apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "W2V_oov = check_coverage(vocab,embeddings_index)\n",
    "W2V_oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 61.05% of vocab\n",
      "Found embeddings for  91.09% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('MH370', 99),\n",
       " ('\\x89ÛÒ', 93),\n",
       " ('\\x89Û', 90),\n",
       " ('\\x89ÛÓ', 58),\n",
       " ('prebreak', 41),\n",
       " ('re\\x89Û', 37),\n",
       " ('Soudelor', 31),\n",
       " ('bestnaijamade', 30),\n",
       " ('TyphoonDevastated', 30),\n",
       " ('\\x89Û_', 28),\n",
       " ('Funtenna', 26),\n",
       " ('\\x89ÛÏWhen', 24),\n",
       " ('don\\x89Ûªt', 23),\n",
       " ('UTC20150805', 17),\n",
       " ('SensorSenso', 17),\n",
       " ('I\\x89Ûªm', 16),\n",
       " ('selfimage', 16),\n",
       " ('China\\x89Ûªs', 15),\n",
       " ('Linkury', 15),\n",
       " ('mhtw4fnet', 15)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GV_path = np.load('../embeddings/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)\n",
    "GV_oov = check_coverage(vocab, GV_path)\n",
    "GV_oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.94% of vocab\n",
      "Found embeddings for  93.81% of all text\n"
     ]
    }
   ],
   "source": [
    "#cleaning\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: clean(x))\n",
    "\n",
    "sentences = df[\"text\"].apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "GV_oov = check_coverage(vocab, GV_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.81% of vocab\n",
      "Found embeddings for  90.96% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('\\x89ÛÒ', 93),\n",
       " ('??', 90),\n",
       " ('\\x89Û', 90),\n",
       " ('\\x89ÛÓ', 58),\n",
       " ('????', 43),\n",
       " ('prebreak', 41),\n",
       " ('re\\x89Û', 37),\n",
       " ('@YouTube', 33),\n",
       " ('Soudelor', 31),\n",
       " ('bestnaijamade', 30),\n",
       " ('TyphoonDevastated', 30),\n",
       " ('\\x89Û_', 28),\n",
       " ('Funtenna', 26),\n",
       " ('\\x89ÛÏWhen', 24),\n",
       " ('??????', 23),\n",
       " ('don\\x89Ûªt', 23),\n",
       " ('UTC20150805', 17),\n",
       " ('SensorSenso', 17),\n",
       " ('I\\x89Ûªm', 16),\n",
       " ('gtgt', 16)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_path = np.load('../embeddings/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', allow_pickle=True)\n",
    "FT_oov = check_coverage(vocab, FT_path)\n",
    "FT_oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 69.20% of vocab\n",
      "Found embeddings for  93.93% of all text\n"
     ]
    }
   ],
   "source": [
    "#cleaning\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: clean(x))\n",
    "\n",
    "sentences = df[\"text\"].apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "FT_oov = check_coverage(vocab, FT_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
